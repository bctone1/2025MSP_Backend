from langchain_service.llms.setup import get_llm
import core.config as config

'''if __name__ == "__main__":
    # OpenAI ì œê³µì í…ŒìŠ¤íŠ¸
    try:
        print("=== OpenAI í…ŒìŠ¤íŠ¸ ===")
        llm_openai = get_llm(provider="openai", model="gpt-3.5-turbo")
        print("âœ… OpenAI LLM ê°ì²´ ìƒì„± ì„±ê³µ!")
        print("ê°ì²´ íƒ€ì…:", type(llm_openai))

        # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
        openai_response = llm_openai.invoke("ì•ˆë…•í•˜ì„¸ìš”! ë‹¹ì‹ ì€ ëˆ„êµ¬ì‹ ê°€ìš”?")
        print("ğŸ¯ OpenAI ì‘ë‹µ:", openai_response)
    except Exception as e:
        print("âŒ OpenAI ì—ëŸ¬ ë°œìƒ:", e)

    # Anthropic ì œê³µì í…ŒìŠ¤íŠ¸
    try:
        print("\n=== Anthropic í…ŒìŠ¤íŠ¸ ===")
        llm_anthropic = get_llm(provider="anthropic")
        print("âœ… Anthropic LLM ê°ì²´ ìƒì„± ì„±ê³µ!")
        print("ê°ì²´ íƒ€ì…:", type(llm_anthropic))

        # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
        anthropic_response = llm_anthropic.invoke("ì•ˆë…•í•˜ì„¸ìš”! ë‹¹ì‹ ì€ ëˆ„êµ¬ì‹ ê°€ìš”?")
        print("ğŸ¯ Anthropic ì‘ë‹µ:", anthropic_response)
    except Exception as e:
        print("âŒ Anthropic ì—ëŸ¬ ë°œìƒ:", e)
'''